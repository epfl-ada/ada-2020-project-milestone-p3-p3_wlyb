{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project idea and description...\n",
    "\n",
    "The original paper explores the hypothesis of chilling effects being present in people's behaviour on Wikipedia, that's to say whether people will tend to be less active in sensitive domains knowing that they are being surveilled by the government. And the conclusion was that such an effect does indeed exist.\n",
    "\n",
    "We aim in our extension to see whether those effects extend to Twitter.\n",
    "\n",
    "The logic behind our choice is that Twitter is a platform of discussion more than it is a platform of learning. So the behaviour of people should be different than on Wikipedia.\n",
    "We also know that people tend to get really vocal on Twitter, so we expect people to actually be more active in talking about those hot-topics as time progresses, especially given that the years 2013-2014 have seen a lot of terrorist-related activity throughout the world."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data aquisition\n",
    "\n",
    "At first we need to obtain tweets containing the same keywords chosen in the original paper, to do so we use the Python module `Twint` (not to be confused with the Swiss payment method) to retrieve those tweets for our selected dates.\n",
    "We only focus on Tweets in English to match the scope of the original paper.\n",
    "\n",
    "We need to get all the following keywords from twitter :\n",
    "\n",
    "* abu sayyaf\n",
    "* afghanistan\n",
    "* agro\n",
    "* al-qaeda\n",
    "* al-qaeda in the arabian peninsula\n",
    "* al-qaeda in the islamic maghreb\n",
    "* al-shabaab\n",
    "* ammonium nitrate\n",
    "* attack\n",
    "* biological weapon\n",
    "* car bomb\n",
    "* chemical weapon\n",
    "* conventional weapon\n",
    "* dirty bomb\n",
    "* eco-terrorism\n",
    "* environmental terrorism\n",
    "* euskadi ta askatasuna\n",
    "* extremism\n",
    "* farc\n",
    "* fundamentalism,\n",
    "* hamas\n",
    "* hezbollah\n",
    "* improvised explosive device\n",
    "* iran\n",
    "* iraq\n",
    "* irish republican army\n",
    "* islamist\n",
    "* jihad\n",
    "* nationalism\n",
    "* nigeria\n",
    "* nuclear\n",
    "* nuclear enrichment\n",
    "* pakistan\n",
    "* palestine liberation front\n",
    "* pirates\n",
    "* plo\n",
    "* political radicalism\n",
    "* recruitment\n",
    "* somalia\n",
    "* suicide attack\n",
    "* suicide bomber\n",
    "* taliban\n",
    "* tamil tigers\n",
    "* tehrik-i-taliban pakistan\n",
    "* terror\n",
    "* terrorism\n",
    "* weapons-grade\n",
    "* yemen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping script\n",
    "\n",
    "Bellow we demonstrate the script that we used to retreive the data using the `twint` module. As the scripts were run in a command line fashion we used the `argparse` module to give command line style arguments to the script like start/end dates and keywords. This script could then be executed on multiple machines to get the full 47 keyword datasets.\n",
    "\n",
    "Example of the command : `python3 scraper.py -q \"abu sayyaf\"` or with an end date `python3 scraper.py -q \"abu sayyaf\" -e 2012-05-06`\n",
    "\n",
    "```python\n",
    "import twint\n",
    "import argparse\n",
    "\n",
    "# get arguments from command line\n",
    "# only the keyword is required, the other have default values \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('-q', '--query', type=str, required=True)\n",
    "parser.add_argument('-s', '--start', type=str, default='2012-01-01')\n",
    "parser.add_argument('-e', '--end', type=str, default='2014-09-01')\n",
    "args = parser.parse_args()\n",
    "\n",
    "config = twint.Config()\n",
    "config.Limit = 5\n",
    "config.Hide_output = False\n",
    "config.Lang = \"en\"\n",
    "config.Since = args.start\n",
    "config.Until = args.end\n",
    "config.Store_csv = True\n",
    "config.Search = args.query\n",
    "config.Output = \"_\".join([args.query, args.start, args.end]) + \".csv\"\n",
    "# make search\n",
    "print(f'Running search for \"{args.query}\" between {args.start} and {args.end}.')\n",
    "twint.run.Search(config)\n",
    "```\n",
    "\n",
    "We had to restart the scripts often as sometimes the network went down or it was hitting an error. The convinience was that the scraped results are automatically saved to the specified output file so that you don't lose 2 days of computing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monitoring the scraping process\n",
    "\n",
    "Scraping took a lot of time and the dataset we collected became quickly huge. Here are some insights and explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of the tweets\n",
    "\n",
    "### Interrupted time series with regression\n",
    "We will now focus on getting an understanding of the tweet distribution over time, and how the massive revelations of online surveillance in June 2013 might have caused a chilling effect. We will follow the paper's original way of doing the interrupted time series, with regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's see what those big files yield by reading one. We need to first find all the archives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "archive_pathnames = glob.glob('./data/*.gz')\n",
    "print(f\"Found {len(archive_pathnames)} archives\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what is in the first archive ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(archive_pathnames[0])\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a lot of information there. But what we are interested in is the user interactions around a topic that they might the government to track, for example. Therefore, we can count the number of tweets themselves, not any of their content or information, but also the number of likes and rerplies ! Each of these actions can make the user fear such surveillance. Retweets also, but because we also collect the retweets themselves, they are already there ! Therefore, we will first count the number of tweets per month, with the number of likes and retweets added as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only interesting columns and sum them all together by month. We set to parse dates so we can group by month\n",
    "df = pd.read_csv(archive_pathnames[0], usecols=[\"date\", \"likes_count\", \"replies_count\"], parse_dates=[\"date\"], lineterminator='\\n')\n",
    "df[\"tweet_count\"] = 1\n",
    "grouped_df = df.set_index('date').groupby(pd.Grouper(freq='M')).sum()\n",
    "grouped_df[\"user_interactions\"] = grouped_df[\"likes_count\"] +  grouped_df[\"tweet_count\"] +  grouped_df[\"replies_count\"]\n",
    "\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to merge all twitter keyworkds into the same dataframe, therefore we will give the name of the keywork to the column where all values are summed, instead of just user_interactions. Let's extract the name from the archive path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(archive_pathnames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowing what the file path looks like, we can extract the name\n",
    "name = re.search(r\"(?<=data/).*?(?=_full)\", archive_pathnames[0]).group(0)\n",
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just set this name to the column and go to the next zip !\n",
    "grouped_df.rename(columns={\"user_interactions\": name}, inplace=True)\n",
    "grouped_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement a for loop to aggregate all the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(pd.read_csv(archive_pathnames[0], usecols=[\"date\", \"likes_count\"]))\n",
    "monthly_counts = pd.DataFrame([])\n",
    "\n",
    "for archive_pathname in archive_pathnames:\n",
    "    print(f\"Reading {archive_pathname} file\")\n",
    "    df = pd.read_csv(archive_pathname, usecols=[\"date\", \"likes_count\", \"replies_count\"], parse_dates=[\"date\"], lineterminator='\\n')\n",
    "    print(f\"Shape is : {df.shape} \\n\")\n",
    "\n",
    "    df[\"tweet_count\"] = 1\n",
    "    df = df.set_index('date').groupby(pd.Grouper(freq='M')).sum()\n",
    "    df[\"user_interactions\"] = df[\"likes_count\"] +  df[\"tweet_count\"] +  df[\"replies_count\"]\n",
    "    name = re.search(r\"(?<=data/).*?(?=_full)\", archive_pathname).group(0)\n",
    "\n",
    "    df.rename(columns={\"user_interactions\": name}, inplace=True)\n",
    "    \n",
    "    monthly_counts = pd.concat([monthly_counts, df[name]], axis=1)\n",
    "monthly_counts.index = pd.to_datetime(monthly_counts.index) # Make sure it is datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"We have {np.round(monthly_counts.sum().sum()/1000000, 2)} million actions !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporary to get working faster !\n",
    "# monthly_counts.to_csv(\"./monthly_actions.csv\")\n",
    "# monthly_counts = pd.read_csv(\"./monthly_actions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now have a quick glance at all the values we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# define figure\n",
    "fig = plt.figure(figsize=(20, 16))\n",
    "\n",
    "# big frame for main labels\n",
    "fig.add_subplot(111, frameon=False)\n",
    "plt.tick_params(labelcolor='none', top=False, bottom=False, left=False, right=False, pad=60)\n",
    "plt.grid(False)\n",
    "plt.xlabel(\"Months\", fontsize=22)\n",
    "plt.ylabel(\"Views\", fontsize=22)\n",
    "\n",
    "# define number of columns and rows of plot\n",
    "col = 4\n",
    "row = len(monthly_counts.columns)//col\n",
    "\n",
    "# plot all topics\n",
    "ax = fig.subplots(row, col, sharey=False, sharex=True)\n",
    "\n",
    "for i, article_name in enumerate(monthly_counts):\n",
    "    axis = ax[i//col, i%col]\n",
    "    sns.lineplot(data=monthly_counts[f\"{article_name}\"], ax=axis)\n",
    "    axis.set_title(article_name)\n",
    "    plt.setp(axis.get_xticklabels(), rotation=30, horizontalalignment='right')\n",
    "    axis.set_ylabel(\"\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this graph we can look for missing values, or things that look anormal. TODO : discuss the weird things once we have complete data\n",
    "\n",
    "In the original article, they study the period of 32 months from january 2012 to end of august 2014. We will therefore restrict our period to the be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "studied_article_actions = monthly_counts[\"2012-01-01\":\"2014-08-31\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we will create the interrupted time series plot, without regression first :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "all_actions = pd.DataFrame(studied_article_actions.sum(axis=1), columns=[\"actions\"])\n",
    "all_actions[\"month_nb\"] = range(1, 33)\n",
    "\n",
    "after_revelations_month = 17 # 16 first months including June 2013, but index starts at 0 so we add 1 \n",
    "\n",
    "# define figure\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "\n",
    "# big frame for main labels\n",
    "plt.title(\"Monthly actions on all topics\")\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"Total Actions\")\n",
    "plt.xticks(np.arange(0, 33, 2.0))\n",
    "plt.scatter(x=all_actions[\"month_nb\"], y=all_actions.actions)\n",
    "plt.axvline(after_revelations_month+0.5, color='orange', label='Mid June 2013') # Plot a vertical line mid June \n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do the regression, to compare the trend before and after the studied interruption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Do linear regression with ols\n",
    "mod_before = smf.ols(formula='actions ~ month_nb',\n",
    "              data=all_actions[:after_revelations_month])\n",
    "res_before = mod_before.fit()\n",
    "\n",
    "mod_after = smf.ols(formula='actions ~ month_nb',\n",
    "              data=all_actions[after_revelations_month:])\n",
    "res_after = mod_after.fit()\n",
    "\n",
    "before_intercept = res_before.params[0]\n",
    "before_slope = res_before.params[1]\n",
    "\n",
    "after_intercept = res_after.params[0]\n",
    "after_slope = res_after.params[1]\n",
    "\n",
    "print(f\"Before period has intercept={before_intercept} and slope={before_slope}\")\n",
    "\n",
    "print(f\"After period has intercept={after_intercept} and slope={after_slope}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, we will plot both regressions, on each side of the interruption :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define figure\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "# big frame for main labels\n",
    "plt.title(\"Interrupted regression of twitter actions across keywords\")\n",
    "plt.xlabel(\"Months\")\n",
    "plt.ylabel(\"Total actions\")\n",
    "plt.xticks(np.arange(0, 33, 2.0))\n",
    "plt.scatter(x=range(1, 33), y=all_actions.actions)\n",
    "plt.axvline(after_revelations_month+0.5, color='orange', label='Mid June 2013') # Plot a vertical line mid June \n",
    "\n",
    "# Now we'll add the before period regression line\n",
    "plt.plot(all_actions[:after_revelations_month].month_nb, all_actions[:after_revelations_month].month_nb*before_slope+before_intercept, label=\"Trend Pre-June 2013\")\n",
    "\n",
    "# And the after period\n",
    "plt.plot(all_actions[after_revelations_month:].month_nb, all_actions[after_revelations_month:].month_nb*after_slope+after_intercept, label=\"Trend Post-June 2013\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m59",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m59"
  },
  "kernelspec": {
   "display_name": "PyCharm (ADA2020)",
   "language": "python",
   "name": "pycharm-465ab897"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
